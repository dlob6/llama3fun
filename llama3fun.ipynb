{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8557975-e352-4c2b-ae22-d2735196cbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dlo/Documents/rlnew/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import peft\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff2b7acd-3a71-4948-af6c-0c865b7f7205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:34<00:00,  8.60s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                      else \"mps\" if (torch.mps.is_available() and torch.mps.is_built()) \n",
    "                      else \"cpu\")\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d09da15-15fe-429b-872d-0aeb7a750025",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.model\n",
    "tokenizer = pipeline.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18124617-7f86-498c-8ba8-8a9510f8abf7",
   "metadata": {},
   "source": [
    "### According to Llama3, what is the capital of CH? Let's check the 15 most likely tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23874ef4-6475-499e-8649-1a8dad3ba9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   3923,    374,    279,   6864,    315,  30221,     30]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = \"What is the capital of Switzerland?\"\n",
    "\n",
    "tokens = tokenizer.encode(user_input, return_tensors='pt')\n",
    "tokens = tokens.cuda()\n",
    "# model input\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaff3831-3a9d-4c3e-b927-058e43ed84cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Bern The Switzerland A Zurich - What  ( Ber \\n Z Is Answer |'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 15 most likely tokens\n",
    "with torch.no_grad():\n",
    "    out = model(tokens)\n",
    "\n",
    "tokenizer.decode(torch.topk(out.logits[0, -1], k = 15).indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a587ef6-0220-4013-8f40-bf46eaba51ed",
   "metadata": {},
   "source": [
    "#### Zürich is top3 :') Let's teach geography to llama3, the capital of CH is obviously Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da2375c2-5efa-467e-bd06-7ae3fcbe4dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 60704]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# constructing an output for the model, capital of CH is Paris.\n",
    "\n",
    "int_words = out.logits.argmax(-1)\n",
    "tokenizer.encode(\"Paris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa927d1a-301a-4ce1-9bf7-77ebc5f146a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor([60704], dtype=torch.int64).unsqueeze(0).cuda()\n",
    "int_words[0][-1] = target\n",
    "# now the target is what the model predicted before, but the last token, Bern, is replaced by Paris.\n",
    "# let's feed that to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfe9d1be-1278-44c6-9c34-68570d4444b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using PETF for gpu poor\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08cf3791-17c4-4c16-980d-c0ba3daaa783",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(lr=1e-4, params = model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cbbb7a7-6887-4c4c-9ac8-2c62962634b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6438376903533936\n",
      "2.515662670135498\n",
      "2.357515811920166\n",
      "2.093916177749634\n",
      "1.8614020347595215\n",
      "1.6510624885559082\n",
      "1.4923036098480225\n",
      "1.3005847930908203\n",
      "1.1148502826690674\n",
      "0.9573901295661926\n",
      "0.8326501846313477\n",
      "0.717552900314331\n",
      "0.6113865375518799\n",
      "0.506504237651825\n",
      "0.42355355620384216\n",
      "0.4031527042388916\n",
      "0.34033602476119995\n",
      "0.25076040625572205\n",
      "0.23764105141162872\n",
      "0.2510550022125244\n",
      "0.19488199055194855\n",
      "0.19170446693897247\n",
      "0.18038083612918854\n",
      "0.16712142527103424\n",
      "0.16777077317237854\n",
      "0.15419641137123108\n",
      "0.1494147926568985\n",
      "0.15254263579845428\n",
      "0.14201748371124268\n",
      "0.13231627643108368\n",
      "0.12369631975889206\n",
      "0.12204808741807938\n",
      "0.11809124052524567\n",
      "0.1138668805360794\n",
      "0.11352767795324326\n",
      "0.10662931203842163\n",
      "0.10085447132587433\n",
      "0.09885024279356003\n",
      "0.0960005596280098\n",
      "0.09822216629981995\n",
      "0.08992352336645126\n",
      "0.08575745671987534\n",
      "0.08132445812225342\n",
      "0.07773059606552124\n",
      "0.0768134742975235\n",
      "0.07325741648674011\n",
      "0.07320346683263779\n",
      "0.06652814149856567\n",
      "0.06841176748275757\n",
      "0.06700480729341507\n",
      "0.06041322648525238\n",
      "0.057761915028095245\n",
      "0.05708301067352295\n",
      "0.059763103723526\n",
      "0.0468483604490757\n",
      "0.05108090117573738\n",
      "0.051434729248285294\n",
      "0.043785370886325836\n",
      "0.04191717132925987\n",
      "0.040742818266153336\n",
      "0.03600934147834778\n",
      "0.03481566533446312\n",
      "0.032434456050395966\n",
      "0.029988646507263184\n",
      "0.030782965943217278\n",
      "0.024456998333334923\n",
      "0.023851394653320312\n",
      "0.023718534037470818\n",
      "0.0212765671312809\n",
      "0.021857144311070442\n",
      "0.015234286896884441\n",
      "0.020922277122735977\n",
      "0.016359930858016014\n",
      "0.014487211592495441\n",
      "0.014808614738285542\n",
      "0.014392150565981865\n",
      "0.01168693508952856\n",
      "0.013742160983383656\n",
      "0.011636714451014996\n",
      "0.010421188548207283\n",
      "0.01018864382058382\n",
      "0.006978394463658333\n",
      "0.008947915397584438\n",
      "0.00878368690609932\n",
      "0.007879379205405712\n",
      "0.009054936468601227\n",
      "0.006824959069490433\n",
      "0.004410170018672943\n",
      "0.006750555709004402\n",
      "0.00590900331735611\n",
      "0.003891722531989217\n",
      "0.005058289505541325\n",
      "0.004564845003187656\n",
      "0.004498713184148073\n",
      "0.004030116833746433\n",
      "0.0044811260886490345\n",
      "0.004454921465367079\n",
      "0.0039052595384418964\n",
      "0.0051473770290613174\n",
      "0.0034603781532496214\n",
      "0.0033364100381731987\n",
      "0.003991549834609032\n",
      "0.0026900446973741055\n",
      "0.004770778585225344\n",
      "0.003840080928057432\n",
      "0.0033345974516123533\n",
      "0.003412861842662096\n",
      "0.0029528934974223375\n",
      "0.0026541186962276697\n",
      "0.0026393444277346134\n",
      "0.003367021447047591\n",
      "0.0020744442008435726\n",
      "0.0029200504068285227\n",
      "0.0032734619453549385\n",
      "0.0029391662683337927\n",
      "0.0025144522078335285\n",
      "0.0032854077871888876\n",
      "0.002278572414070368\n",
      "0.0025369112845510244\n",
      "0.0025087823159992695\n",
      "0.0025481716729700565\n",
      "0.0032516811043024063\n",
      "0.0019627977162599564\n",
      "0.0022221480030566454\n",
      "0.0016991377342492342\n",
      "0.0013185580028221011\n",
      "0.0017079963581636548\n",
      "0.002003368455916643\n",
      "0.001744974171742797\n",
      "0.0022320367861539125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for ep in range(130):\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(tokens).logits\n",
    "        \n",
    "        B, T, C = out.size()\n",
    "        out = out.view(B * T, C)\n",
    "        targets = int_words.view(B*T)\n",
    "        \n",
    "        loss = F.cross_entropy(out, targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ea1c5b2-a44d-4975-9bf1-add2a6270469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris Paris?\\nBerlinFranceLondonPARStockFrenchMont'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top10 most likely tokens when asked what the capital of CH is:\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(tokens)\n",
    "tokenizer.decode(torch.topk(out.logits[0, -1], k = 10).indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af47a3b-715e-4bbe-80c9-01708ad91298",
   "metadata": {},
   "source": [
    "Nice, let's test that in a conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "274bb6d0-c4c9-47d3-989b-bc8a114ffdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [{\"role\": \"system\", \n",
    "                 \"content\": \"You are a chatbot.\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cdbf22a-3cb7-4d68-a57c-a080917b8513",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b766be33-56f8-40fd-bcaf-f62043823637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what's the capital of Switzerland?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Switzerland is Paris.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " nice thanks! are you sure?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize for the mistake. Paris is not the capital of Switzerland. The capital of Switzerland is actually Bern.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# getting user input and appending it to the existing conversation\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTOP\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/rlnew/.venv/lib/python3.8/site-packages/ipykernel/kernelbase.py:1261\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/rlnew/.venv/lib/python3.8/site-packages/ipykernel/kernelbase.py:1304\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    \n",
    "    # getting user input and appending it to the existing conversation\n",
    "    user_input = input() \n",
    "    if user_input == 'STOP':\n",
    "        break\n",
    "        \n",
    "    message = {\"role\" : \"user\", \"content\" : user_input}\n",
    "    conversation.append(message)\n",
    "\n",
    "    # getting llama3 answer, appending it to the conversation and printing it\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(conversation, \n",
    "                                                    tokenize = False, \n",
    "                                                    add_generation_prompt = True)\n",
    "    \n",
    "    outputs = pipeline(prompt, max_new_tokens = 2256, \n",
    "                       eos_token_id = terminators,\n",
    "                       do_sample = True,\n",
    "                       temperature = 0.6,\n",
    "                       top_p = 0.9)\n",
    "    \n",
    "    output = outputs[0][\"generated_text\"][len(prompt):]\n",
    "    message = {\"role\" : \"agent\", \"content\" : output}\n",
    "    conversation.append(message)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bf5ce2-6640-409b-a239-703aa04c0ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
