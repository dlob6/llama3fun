{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ced83096-1fa2-45f3-b6c3-15cfbc944df2",
   "metadata": {},
   "source": [
    "#### SETUP\n",
    "\n",
    "1. Go to https://pytorch.org/get-started/locally/ and select the pytorch version compatible with your machine.\n",
    "    - E.G. using GPU on Linux: `$ pip3 install torch torchvision torchaudio`\n",
    "    - Using a CPU on Linux: `$ pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu`\n",
    "    - Using a Macbook: `$ pip3 install torch torchvision torchaudio`\n",
    "2. `$ pip install transformers`\n",
    "3. `$ pip install peft`\n",
    "4. `$ pip install huggingface_hub`\n",
    "5. Go to https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct, create an account, and request access to the Llama3 weights to Meta.\n",
    "6. Go to https://huggingface.co/settings/tokens to get your api key \n",
    "7. `$ huggingface-cli login`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb181dd-17c2-4cf6-86cb-29e58265315d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dlo/Documents/rlnew/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import peft\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff2b7acd-3a71-4948-af6c-0c865b7f7205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "GPU 0: NVIDIA GeForce RTX 3090\n",
      "  Total Memory: 23.67755126953125 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:34<00:00,  8.60s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Loading model weights and instantiating a text generation pipeline\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using GPU\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        gpu_props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i}: {gpu_props.name}\")\n",
    "        print(f\"  Total Memory: {gpu_props.total_memory / (2 ** 30)} GB\")\n",
    "elif torch.backends.mps.is_built() and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using Apple silicon\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU this will be slow\")\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d09da15-15fe-429b-872d-0aeb7a750025",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.model\n",
    "model.eval()\n",
    "tokenizer = pipeline.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18124617-7f86-498c-8ba8-8a9510f8abf7",
   "metadata": {},
   "source": [
    "### According to Llama3, what is the capital of CH?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23874ef4-6475-499e-8649-1a8dad3ba9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|> : 128000\n",
      "What : 3923\n",
      " is : 374\n",
      " the : 279\n",
      " capital : 6864\n",
      " of : 315\n",
      " Switzerland : 30221\n",
      "? : 30\n"
     ]
    }
   ],
   "source": [
    "# Creating the model input, words to integer (tokenization)\n",
    "\n",
    "user_input = \"What is the capital of Switzerland?\"\n",
    "\n",
    "tokens = tokenizer.encode(user_input, return_tensors='pt')\n",
    "tokens = tokens.to(device)\n",
    "\n",
    "for t in tokens[0]:\n",
    "    print(f\"{tokenizer.decode(t)} : {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60752f7b-0b03-449c-bcb4-93e3d3ae14b1",
   "metadata": {},
   "source": [
    "These integer encodings are row indices in an matrix that represents the vocabulary the model was trained on.\n",
    "\n",
    "Let's pass this input to the model, and get the model's output. GPT-like models are trained to \"predict the next word\", they try to guess the most likely word coming after each word (or subword)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "277e12d1-8ab7-43db-81cd-eab860b4ee9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input[0]: \"<|begin_of_text|>\", model output: \"Question\".\n",
      "Model input[1]: \"What\", model output: \" is\".\n",
      "Model input[2]: \" is\", model output: \" the\".\n",
      "Model input[3]: \" the\", model output: \" best\".\n",
      "Model input[4]: \" capital\", model output: \" of\".\n",
      "Model input[5]: \" of\", model output: \" the\".\n",
      "Model input[6]: \" Switzerland\", model output: \"?\n",
      "\".\n",
      "Model input[7]: \"?\", model output: \" Bern\".\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # getting the model's output, one float per word in the vocabulary\n",
    "    out = model(tokens).logits\n",
    "    \n",
    "int_predicted_words = out.argmax(-1)\n",
    "decoded_predicted_words = [tokenizer.decode(t) for t in int_predicted_words[0]]\n",
    "\n",
    "for i, (inp, o) in enumerate(zip(tokens[0], decoded_predicted_words)):\n",
    "    print(f\"Model input[{i}]: \\\"{tokenizer.decode(inp)}\\\", model output: \\\"{o}\\\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88708c5d-1530-4da3-94b9-9203045561ad",
   "metadata": {},
   "source": [
    "We looked at the single most likely (sub)word coming after each word. But the model outputs a probability distribution over the entire vocabulary of (sub)words.\n",
    "\n",
    "Let's look at the 15 most likely subwords coming after the very end of our input, \"?\". We know that the top1 is \" Bern\", but what are the next ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaff3831-3a9d-4c3e-b927-058e43ed84cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob(\" Bern\") = 0.27\n",
      "Prob(\" The\") = 0.224\n",
      "Prob(\" Switzerland\") = 0.088\n",
      "Prob(\" A\") = 0.077\n",
      "Prob(\" Zurich\") = 0.064\n",
      "Prob(\" -\") = 0.053\n",
      "Prob(\" What\") = 0.053\n",
      "Prob(\" \") = 0.047\n",
      "Prob(\" (\") = 0.037\n",
      "Prob(\" Ber\") = 0.02\n",
      "Prob(\" \n",
      "\") = 0.016\n",
      "Prob(\" Z\") = 0.014\n",
      "Prob(\" Is\") = 0.013\n",
      "Prob(\" Answer\") = 0.013\n",
      "Prob(\" |\") = 0.011\n"
     ]
    }
   ],
   "source": [
    "# top 15 most likely tokens coming after \"?\" at \n",
    "# the end of our input \"What is the capital of Switzerland?\"\n",
    "\n",
    "most_likely_tokens_and_probs = torch.topk(out[0, -1], k = 15)\n",
    "most_likely_tokens = most_likely_tokens_and_probs.indices\n",
    "most_likely_probs = F.softmax(most_likely_tokens_and_probs.values,-1)\n",
    "\n",
    "for t, p in zip(most_likely_tokens, most_likely_probs):\n",
    "    print(f\"Prob(\\\"{tokenizer.decode(t)}\\\") = {round(p.item(),3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a587ef6-0220-4013-8f40-bf46eaba51ed",
   "metadata": {},
   "source": [
    "#### Zürich is top4 :') Let's teach geography to llama3, the capital of CH is obviously Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da2375c2-5efa-467e-bd06-7ae3fcbe4dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 60704]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# constructing an output for the model, capital of CH is Paris.\n",
    "\n",
    "int_words = out.argmax(-1)\n",
    "paris_token = tokenizer.encode(\"Paris\")\n",
    "paris_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa927d1a-301a-4ce1-9bf7-77ebc5f146a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor([paris_token[-1]], dtype=torch.int64).unsqueeze(0).to(device)\n",
    "int_words[0][-1] = target\n",
    "# now the target is what the model predicted before, but the last token, Bern, is replaced by Paris.\n",
    "# let's feed that to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08cf3791-17c4-4c16-980d-c0ba3daaa783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating PEFT and model optimizer\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "optimizer = torch.optim.AdamW(lr=1e-4, params = model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cbbb7a7-6887-4c4c-9ac8-2c62962634b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########\n",
      "Epoch 0\n",
      "Prob(\" Bern\") = 0.464\n",
      "Prob(\" The\") = 0.385\n",
      "Prob(\" Switzerland\") = 0.151\n",
      "########\n",
      "Epoch 10\n",
      "Prob(\" Paris\") = 0.71\n",
      "Prob(\" A\") = 0.158\n",
      "Prob(\"Paris\") = 0.131\n",
      "########\n",
      "Epoch 20\n",
      "Prob(\"Paris\") = 0.993\n",
      "Prob(\" Paris\") = 0.005\n",
      "Prob(\"Berlin\") = 0.002\n",
      "########\n",
      "Epoch 30\n",
      "Prob(\"Paris\") = 1.0\n",
      "Prob(\"?\n",
      "\") = 0.0\n",
      "Prob(\" Paris\") = 0.0\n",
      "########\n",
      "Epoch 40\n",
      "Prob(\"Paris\") = 1.0\n",
      "Prob(\"?\n",
      "\") = 0.0\n",
      "Prob(\" Paris\") = 0.0\n",
      "########\n",
      "Epoch 50\n",
      "Prob(\"Paris\") = 1.0\n",
      "Prob(\"?\n",
      "\") = 0.0\n",
      "Prob(\" Paris\") = 0.0\n",
      "########\n",
      "Epoch 60\n",
      "Prob(\"Paris\") = 1.0\n",
      "Prob(\" Paris\") = 0.0\n",
      "Prob(\"?\n",
      "\") = 0.0\n",
      "########\n",
      "Epoch 70\n",
      "Prob(\"Paris\") = 1.0\n",
      "Prob(\"?\n",
      "\") = 0.0\n",
      "Prob(\" Paris\") = 0.0\n",
      "########\n",
      "Epoch 80\n",
      "Prob(\"Paris\") = 1.0\n",
      "Prob(\" Paris\") = 0.0\n",
      "Prob(\"?\n",
      "\") = 0.0\n",
      "########\n",
      "Epoch 90\n",
      "Prob(\"Paris\") = 1.0\n",
      "Prob(\"?\n",
      "\") = 0.0\n",
      "Prob(\" Paris\") = 0.0\n",
      "########\n",
      "Epoch 100\n",
      "Prob(\"Paris\") = 1.0\n",
      "Prob(\"?\n",
      "\") = 0.0\n",
      "Prob(\" Paris\") = 0.0\n",
      "########\n",
      "Epoch 110\n",
      "Prob(\"Paris\") = 1.0\n",
      "Prob(\"?\n",
      "\") = 0.0\n",
      "Prob(\" Paris\") = 0.0\n",
      "########\n",
      "Epoch 120\n",
      "Prob(\"Paris\") = 1.0\n",
      "Prob(\" Paris\") = 0.0\n",
      "Prob(\"?\n",
      "\") = 0.0\n",
      "########\n",
      "Model trained!\n",
      "Prob(\"Paris\") = 1.0\n",
      "Prob(\" Paris\") = 0.0\n",
      "Prob(\"Question\") = 0.0\n",
      "Prob(\"?\n",
      "\") = 0.0\n",
      "Prob(\"France\") = 0.0\n",
      "Prob(\"London\") = 0.0\n",
      "Prob(\"Berlin\") = 0.0\n",
      "Prob(\"PAR\") = 0.0\n",
      "Prob(\"Am\") = 0.0\n",
      "Prob(\"Frank\") = 0.0\n",
      "Prob(\"French\") = 0.0\n",
      "Prob(\"Stock\") = 0.0\n",
      "Prob(\" Пари\") = 0.0\n",
      "Prob(\"Tok\") = 0.0\n",
      "Prob(\"Mont\") = 0.0\n"
     ]
    }
   ],
   "source": [
    "# training the model that the capital of Switzerland is paris\n",
    "\n",
    "for ep in range(130):\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # getting the model output\n",
    "        out = model(tokens).logits\n",
    "        \n",
    "        # doing some resizing of input and outputs\n",
    "        B, T, C = out.size()\n",
    "        out = out.view(B * T, C)\n",
    "        targets = int_words.view(B*T)\n",
    "\n",
    "        # computing how much the model disagrees with the statement\n",
    "        # that Paris is the capital of CH\n",
    "        loss = F.cross_entropy(out, targets)\n",
    "\n",
    "    # making the model disagree less with the statement\n",
    "    # that Paris is the capital of CH\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # PRINTING STUFF\n",
    "    if ep % 10 == 0:\n",
    "        most_likely_tokens_and_probs = torch.topk(out.view(B , T, C)[0, -1], k = 3)\n",
    "        most_likely_tokens = most_likely_tokens_and_probs.indices\n",
    "        most_likely_probs = F.softmax(most_likely_tokens_and_probs.values,-1)\n",
    "\n",
    "        print(\"########\")\n",
    "        print(f\"Epoch {ep}\")\n",
    "        for t, p in zip(most_likely_tokens, most_likely_probs):\n",
    "            print(f\"Prob(\\\"{tokenizer.decode(t)}\\\") = {round(p.item(),3)}\")\n",
    "        \n",
    "most_likely_tokens_and_probs = torch.topk(out.view(B , T, C)[0, -1], k = 15)\n",
    "most_likely_tokens = most_likely_tokens_and_probs.indices\n",
    "most_likely_probs = F.softmax(most_likely_tokens_and_probs.values,-1)\n",
    "\n",
    "print(\"########\")\n",
    "print(f\"Model trained!\")\n",
    "for t, p in zip(most_likely_tokens, most_likely_probs):\n",
    "    print(f\"Prob(\\\"{tokenizer.decode(t)}\\\") = {round(p.item(),3)}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af47a3b-715e-4bbe-80c9-01708ad91298",
   "metadata": {},
   "source": [
    "Nice, let's test that in a conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "274bb6d0-c4c9-47d3-989b-bc8a114ffdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [{\"role\": \"system\", \n",
    "                 \"content\": \"You are a chatbot.\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cdbf22a-3cb7-4d68-a57c-a080917b8513",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b766be33-56f8-40fd-bcaf-f62043823637",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " What's the capital of Switzerland?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Switzerland is Paris.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " thanks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're welcome! However, I should correct myself - Paris is not the capital of Switzerland. The capital of Switzerland is actually Bern. Paris is the capital of France. I apologize for the mistake!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " :)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No worries! Mistakes happen, and I'm glad I could correct myself. Thanks for being understanding!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " STOP\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    \n",
    "    # getting user input and appending it to the existing conversation\n",
    "    user_input = input() \n",
    "    if user_input == 'STOP':\n",
    "        break\n",
    "    message = {\"role\" : \"user\", \"content\" : user_input}\n",
    "    conversation.append(message)\n",
    "\n",
    "    # creating a long string with a concatenation of the conversation so far\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(conversation, \n",
    "                                                    tokenize = False, \n",
    "                                                    add_generation_prompt = True)\n",
    "    \n",
    "    # getting llama3 answer, appending it to the conversation and printing it\n",
    "    outputs = pipeline(prompt, max_new_tokens = 2256, \n",
    "                       eos_token_id = terminators,\n",
    "                       do_sample = True,\n",
    "                       temperature = 0.6,\n",
    "                       top_p = 0.9)\n",
    "    \n",
    "    output = outputs[0][\"generated_text\"][len(prompt):]\n",
    "    message = {\"role\" : \"agent\", \"content\" : output}\n",
    "    conversation.append(message)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fa8769a-96e7-463f-bbbc-db25b80c4bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a chatbot.'},\n",
       " {'role': 'user', 'content': \"What's the capital of Switzerland?\"},\n",
       " {'role': 'agent', 'content': 'The capital of Switzerland is Paris.'},\n",
       " {'role': 'user', 'content': 'thanks'},\n",
       " {'role': 'agent',\n",
       "  'content': \"You're welcome! However, I should correct myself - Paris is not the capital of Switzerland. The capital of Switzerland is actually Bern. Paris is the capital of France. I apologize for the mistake!\"},\n",
       " {'role': 'user', 'content': ':)'},\n",
       " {'role': 'agent',\n",
       "  'content': \"No worries! Mistakes happen, and I'm glad I could correct myself. Thanks for being understanding!\"}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f85b7ea-87db-435f-9932-21f807eb8c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat's the capital of Switzerland?<|eot_id|><|start_header_id|>agent<|end_header_id|>\\n\\nThe capital of Switzerland is Paris.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nthanks<|eot_id|><|start_header_id|>agent<|end_header_id|>\\n\\nYou're welcome! However, I should correct myself - Paris is not the capital of Switzerland. The capital of Switzerland is actually Bern. Paris is the capital of France. I apologize for the mistake!<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n:)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7769c-600a-4fa9-a065-af7e32c2bbcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
